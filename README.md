# AdaBoost on MNIST â€” Step-by-Step (EN/ES)

> A very clear walkthrough of this project for high-school students. Each paragraph is explained in **English** ğŸ‡ºğŸ‡¸ and **Spanish** ğŸ‡ªğŸ‡¸.

---

## 1) What this project does

- ğŸ‡ºğŸ‡¸ **English:** This project trains a simple **AdaBoost** classifier to recognize handwritten digits from the **MNIST** dataset. It also includes a small **MLP (neural network)** as a reference. You can run binary â€œone digit vs the rest,â€ full **multiclass** (0â€“9 with one-vs-all), and draw **performance charts**.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Este proyecto entrena un clasificador **AdaBoost** sencillo para reconocer dÃ­gitos escritos a mano del conjunto **MNIST**. TambiÃ©n incluye una pequeÃ±a **MLP (red neuronal)** como referencia. Puedes ejecutar el caso binario â€œun dÃ­gito contra el restoâ€, el caso **multiclase** (0â€“9 con uno-contra-todos) y dibujar **grÃ¡ficas de rendimiento**.

---

## 2) Files & structure

- ğŸ‡ºğŸ‡¸ **English:**  
  - `Jorge_Moreno_Ozores.py` â€” all the code (AdaBoost, MLP, plots, helpers).  
  - No extra data files; MNIST downloads automatically via `tf.keras`.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - `Jorge_Moreno_Ozores.py` â€” todo el cÃ³digo (AdaBoost, MLP, grÃ¡ficas, utilidades).  
  - Sin archivos de datos extra; MNIST se descarga automÃ¡ticamente con `tf.keras`.

---

## 3) How to run

- ğŸ‡ºğŸ‡¸ **English:**  
  - Install Python 3.10+ and run:  
    ```bash
    pip install tensorflow matplotlib scikit-learn numpy
    python Jorge_Moreno_Ozores.py
    ```  
  - By default it runs a **binary AdaBoost** for digit **9** and prints accuracy and time.  
  - Uncomment other lines in `__main__` to run the multiclass version, sklearn baseline, MLP, and plots.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - Instala Python 3.10+ y ejecuta:  
    ```bash
    pip install tensorflow matplotlib scikit-learn numpy
    python Jorge_Moreno_Ozores.py
    ```  
  - Por defecto ejecuta **AdaBoost binario** para el dÃ­gito **9** e imprime precisiÃ³n y tiempo.  
  - Descomenta otras lÃ­neas en `__main__` para lanzar la versiÃ³n multiclase, la lÃ­nea base de sklearn, la MLP y las grÃ¡ficas.

---

## 4) Reproducibility (same results every run)

- ğŸ‡ºğŸ‡¸ **English:** We **fix random seeds** for Python, NumPy, and TensorFlow. We also set **TensorFlow flags before importing it** so settings take effect. That means: same inputs â‡’ same outputs.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** **Fijamos semillas** para Python, NumPy y TensorFlow. TambiÃ©n ponemos **las flags de TensorFlow antes de importarlo** para que surtan efecto. AsÃ­: mismas entradas â‡’ mismos resultados.

---

## 5) Key concepts (mini-glossary)

- ğŸ‡ºğŸ‡¸ **English:**  
  - **Classifier:** a program that assigns a label (e.g., â€œthis is a 9â€).  
  - **Feature:** a measurable property of data (here, a pixel value among 784).  
  - **Threshold:** a cut-off value used to split decisions (e.g., pixel < 0.3).  
  - **Polarity (Â±1):** which side of the threshold predicts +1 vs âˆ’1.  
  - **Weighted error:** mistakes counted with sample weights (some examples matter more).  
  - **Alpha (Î±):** the weight given to each weak classifier in AdaBoost.  
  - **Margin:** the sum of (alpha Ã— prediction) across weak classifiers; sign decides the class.  
  - **Seed:** a fixed number to make randomness repeatable.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - **Clasificador:** programa que asigna una etiqueta (p. ej., â€œesto es un 9â€).  
  - **CaracterÃ­stica (feature):** propiedad medible del dato (aquÃ­, un pÃ­xel entre 784).  
  - **Umbral (threshold):** valor de corte para decidir (p. ej., pÃ­xel < 0.3).  
  - **Polaridad (Â±1):** quÃ© lado del umbral predice +1 frente a âˆ’1.  
  - **Error ponderado:** fallos contados con pesos (algunos ejemplos importan mÃ¡s).  
  - **Alfa (Î±):** peso que se da a cada clasificador dÃ©bil en AdaBoost.  
  - **Margen:** suma de (alfa Ã— predicciÃ³n) de los clasificadores dÃ©biles; el signo decide la clase.  
  - **Semilla:** nÃºmero fijo para que el azar sea repetible.

---

## 6) What is AdaBoost (intuitive)

- ğŸ‡ºğŸ‡¸ **English:** AdaBoost builds a **team** of very simple â€œweakâ€ classifiers (here: **decision stumps** that look at just **one feature** and a **threshold**). Each weak classifier is not great alone, but AdaBoost **weights** them (using **alpha**) and **adds** their opinions. Together, they become strong.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** AdaBoost forma un **equipo** de clasificadores â€œdÃ©bilesâ€ muy simples (aquÃ­: **stumps** que miran **una sola caracterÃ­stica** y un **umbral**). Cada dÃ©bil no es muy bueno, pero AdaBoost los **pondera** (con **alfa**) y **suma** sus opiniones. Juntos se vuelven fuertes.

---

## 7) The Decision Stump (our weak classifier)

- ğŸ‡ºğŸ‡¸ **English:** A stump picks **one pixel** (feature), a **threshold** (like 0.27), and a **polarity** (which side is +1). Then it predicts +1 or âˆ’1 for each image based on that single rule.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Un stump elige **un pÃ­xel** (caracterÃ­stica), un **umbral** (como 0.27) y una **polaridad** (quÃ© lado es +1). Luego predice +1 o âˆ’1 para cada imagen con esa Ãºnica regla.

---

## 8) AdaBoost training â€” the `fit` loop (clear steps)

**Round by round (T rounds). In each round we try A random stumps and keep the best one.**

1) **Initialize sample weights (D):**  
   - ğŸ‡ºğŸ‡¸ All training images start with equal weight (each is equally important).  
   - ğŸ‡ªğŸ‡¸ Todas las imÃ¡genes de entrenamiento empiezan con el mismo peso (todas importan igual).

2) **Try A random stumps; pick the best:**  
   - ğŸ‡ºğŸ‡¸ We create A random stumps (random feature, threshold, polarity).  
     For each stump, we predict on all images and compute the **weighted error** (sum weights where itâ€™s wrong). We **keep the stump with the smallest weighted error**.  
   - ğŸ‡ªğŸ‡¸ Creamos A stumps aleatorios (caracterÃ­stica, umbral y polaridad aleatorios).  
     Para cada stump, predecimos en todas las imÃ¡genes y calculamos el **error ponderado** (sumamos pesos en los fallos). **Guardamos el stump con menor error ponderado**.

3) **Compute alpha (Î±) for the best stump:**  
   - ğŸ‡ºğŸ‡¸ We convert the error into a strength with  
     \\( \\alpha = \\tfrac{1}{2}\\log\\frac{1 - e}{e} \\)  
     We **clip** `e` to avoid 0 or 1 (numerical safety).  
   - ğŸ‡ªğŸ‡¸ Convertimos el error en â€œfuerzaâ€ con  
     \\( \\alpha = \\tfrac{1}{2}\\log\\frac{1 - e}{e} \\)  
     Hacemos **clip** de `e` para evitar 0 o 1 (seguridad numÃ©rica).

4) **Update sample weights (D):**  
   - ğŸ‡ºğŸ‡¸ Increase weights of misclassified images and decrease weights of correct ones using  
     \\( D \\leftarrow D \\cdot \\exp(-\\alpha \\cdot y \\cdot \\hat{y}) \\)  
     Then **normalize** D so it sums to 1.  
   - ğŸ‡ªğŸ‡¸ Aumentamos el peso de las imÃ¡genes mal clasificadas y reducimos el de las bien clasificadas con  
     \\( D \\leftarrow D \\cdot \\exp(-\\alpha \\cdot y \\cdot \\hat{y}) \\)  
     DespuÃ©s **normalizamos** D para que sume 1.

5) **Store the best stump:**  
   - ğŸ‡ºğŸ‡¸ Save the stump and its Î±; weâ€™ll use it for prediction later.  
   - ğŸ‡ªğŸ‡¸ Guardamos el stump y su Î±; lo usaremos luego para predecir.

6) **Repeat for T rounds:**  
   - ğŸ‡ºğŸ‡¸ Each round focuses more on the examples that were hard in previous rounds.  
   - ğŸ‡ªğŸ‡¸ Cada ronda se centra mÃ¡s en los ejemplos que fueron difÃ­ciles en rondas anteriores.

---

## 9) How prediction works

- ğŸ‡ºğŸ‡¸ **English:** For a new image, each saved stump makes a prediction in {âˆ’1, +1}. We compute a **margin** = sum over stumps of (Î± Ã— prediction). The final class is the **sign** of the margin. We use a **deterministic tie-break**: if margin = 0, we return **+1** (no zeros).  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Para una nueva imagen, cada stump guardado da una predicciÃ³n en {âˆ’1, +1}. Calculamos el **margen** = suma de (Î± Ã— predicciÃ³n). La clase final es el **signo** del margen. Usamos un **desempate determinista**: si el margen = 0, devolvemos **+1** (sin ceros).

---

## 10) Binary vs Multiclass

- ğŸ‡ºğŸ‡¸ **English:**  
  - **Binary:** Detect one digit (e.g., â€œis it a 9?â€) â‡’ labels are {âˆ’1, +1}.  
  - **Multiclass (0â€“9):** We train 10 binary classifiers (one-vs-all). For a test image, each classifier outputs a **margin**, and we pick the class with the **largest margin**.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - **Binario:** Detectar un dÃ­gito (p. ej., â€œÂ¿es un 9?â€) â‡’ etiquetas {âˆ’1, +1}.  
  - **Multiclase (0â€“9):** Entrenamos 10 clasificadores binarios (uno-contra-todos). Para una imagen, cada clasificador devuelve un **margen** y elegimos la clase con el **margen mÃ¡s grande**.

---

## 11) Safe class balancing (for one-vs-all training)

- ğŸ‡ºğŸ‡¸ **English:** To avoid huge class imbalance (far more â€œnot-9â€ than â€œ9â€), we **balance** the training subset: take all positives and the **same number** of negatives (or as many as exist), then **shuffle** with a fixed seed so itâ€™s **deterministic**.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Para evitar un gran desbalance (muchos mÃ¡s â€œno-9â€ que â€œ9â€), **balanceamos** el subconjunto: tomamos todos los positivos y el **mismo nÃºmero** de negativos (o los que haya), y **barajamos** con semilla fija para que sea **determinista**.

---

## 12) Reference MLP (neural network)

- ğŸ‡ºğŸ‡¸ **English:** We include a small **MLP** (two Dense layers and a softmax) to compare with AdaBoost. Since inputs are already flat (784), we **donâ€™t need a Flatten layer**.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Incluimos una **MLP** pequeÃ±a (dos capas Dense y softmax) para comparar con AdaBoost. Como las entradas ya son planas (784), **no necesitamos capa Flatten**.

---

## 13) Plots & experiments

- ğŸ‡ºğŸ‡¸ **English:** Functions `tarea_1C_graficas_rendimiento`, `tarea_2B_graficas_rendimiento`, and `tarea_2F_graficas_rendimiento` scan hyper-parameters and draw charts of **accuracy** and **training time** so you can see trade-offs.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:** Las funciones `tarea_1C_graficas_rendimiento`, `tarea_2B_graficas_rendimiento` y `tarea_2F_graficas_rendimiento` exploran hiperparÃ¡metros y dibujan grÃ¡ficas de **precisiÃ³n** y **tiempo de entrenamiento** para ver los compromisos.

---

## 14) Troubleshooting (common issues)

- ğŸ‡ºğŸ‡¸ **English:**  
  - If results change between runs, ensure you **didnâ€™t edit seeds** and that you run a **single process**.  
  - If TensorFlow prints many messages, confirm env flags are **set before importing TensorFlow**.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - Si los resultados cambian entre ejecuciones, comprueba que **no cambiaste las semillas** y que ejecutas en **un solo proceso**.  
  - Si TensorFlow imprime muchos mensajes, confirma que las flags se **ponen antes de importar TensorFlow**.

---

## 15) Quick commands

- ğŸ‡ºğŸ‡¸ **English:**  
  - Binary (default in `__main__`):  
    ```bash
    python Jorge_Moreno_Ozores.py
    ```  
  - Multiclass one-vs-all (edit `__main__` to uncomment):  
    ```python
    # tarea_1D_adaboost_multiclase(T=25, A=300, verbose=True)
    ```  
  - MLP reference (edit `__main__` to uncomment):  
    ```python
    # tarea_2D_clasificador_MLP_para_MNIST_con_Keras()
    ```  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  - Binario (por defecto en `__main__`):  
    ```bash
    python Jorge_Moreno_Ozores.py
    ```  
  - Multiclase uno-contra-todos (edita `__main__` y descomenta):  
    ```python
    # tarea_1D_adaboost_multiclase(T=25, A=300, verbose=True)
    ```  
  - MLP de referencia (edita `__main__` y descomenta):  
    ```python
    # tarea_2D_clasificador_MLP_para_MNIST_con_Keras()
    ```

---

## 16) Recent Changes (what we improved & why)

- ğŸ‡ºğŸ‡¸ **English:**  
  1. **Set TF env flags before import** â€” TF reads them at import time; doing it later has no effect (robustness, cleaner logs).  
  2. **Global seeding** (`random`, `numpy`, `tensorflow`, `PYTHONHASHSEED`) â€” ensures **reproducibility** (same results every run).  
  3. **Deterministic TF ops when available** â€” reduces run-to-run noise in GPU/CPU kernels.  
  4. **Stable alpha with error clipping** â€” avoid `log(0)` or `log(âˆ)` when a stump is perfect or terrible (numerical stability).  
  5. **Deterministic sign rule** (`sign_with_tie`, where 0 â‡’ +1) â€” avoids invalid class â€œ0â€ in binary classification (consistency).  
  6. **Safe, deterministic class balancing** â€” never sample more negatives than exist; shuffle with a fixed seed (robustness + reproducibility).  
  7. **Clear weighted-error mask** â€” use a boolean mask `pred != Y` then sum weights (readability, same speed).  
  8. **Remove redundant Flatten in MLP** â€” inputs are already `(n, 784)` (simplicity, tiny efficiency gain).  
  9. **Set `random_state` in sklearn AdaBoost** â€” makes the sklearn baseline reproducible.  
  10. **Avoid shadowing names** (donâ€™t use `tf` as a timer variable) â€” prevents confusion with the TensorFlow module.  
  11. **Unify Keras imports under `tf.keras`** â€” fewer inconsistencies across versions.  
- ğŸ‡ªğŸ‡¸ **EspaÃ±ol:**  
  1. **Flags de TF antes de importar** â€” TF las lee al importarse; ponerlas despuÃ©s no sirve (robustez, logs limpios).  
  2. **Semillas globales** (`random`, `numpy`, `tensorflow`, `PYTHONHASHSEED`) â€” garantiza **reproducibilidad** (mismos resultados).  
  3. **Operaciones deterministas de TF si existen** â€” reduce variaciones entre ejecuciones en GPU/CPU.  
  4. **Alfa estable con clip del error** â€” evita `log(0)` o `log(âˆ)` cuando un stump es perfecto o muy malo (estabilidad numÃ©rica).  
  5. **Regla de signo determinista** (`sign_with_tie`, donde 0 â‡’ +1) â€” evita clase invÃ¡lida â€œ0â€ en binario (consistencia).  
  6. **Balanceo seguro y determinista** â€” nunca pedimos mÃ¡s negativos de los que hay; barajado con semilla (robustez + reproducibilidad).  
  7. **MÃ¡scara clara para error ponderado** â€” usamos `pred != Y` y sumamos pesos (legibilidad, misma velocidad).  
  8. **Quitar Flatten redundante en MLP** â€” la entrada ya es `(n, 784)` (simpleza, ligera eficiencia).  
  9. **`random_state` en AdaBoost de sklearn** â€” hace reproducible la lÃ­nea base.  
  10. **Evitar nombres que se solapan** (no usar `tf` como tiempo) â€” evita confusiones con el mÃ³dulo TensorFlow.  
  11. **Unificar imports de Keras en `tf.keras`** â€” menos inconsistencias entre versiones.

---

**Enjoy experimenting! / Â¡Disfruta experimentando!**
